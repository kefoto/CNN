{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kefoto/CNN/blob/main/14-Langfuse-connection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Inference with Sep28k and Langfuse and designed prompt\n",
        "> Using Qwen2.5-audio-7B-Instruct for audio analysis & Mistral-7B-Instruct-v0.3 and instructor wrapper for structured output\n",
        "\n",
        "require 30GB of VRAM since Mistral-7B requires 15 GB of Vram\n",
        "\n",
        "In this notebook, we read from the Sep28k dataset stored on the `vanderbilt-dsi` Huggingface account. We leverage 2 audio examples and ask a few questions about the audio contained. Below are the results of this exploration.\n",
        "\n",
        "See [Issue #11](https://github.com/vanderbilt-data-science/stutter-models/issues/11) for more information on setting up your Huggingface Token and adding this in Google Colab."
      ],
      "metadata": {
        "id": "_lXk8cd420B2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KrRvHmzGL0h-"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"transformers>=4.43\" accelerate einops librosa soundfile sentencepiece torchaudio torch torchcodec==0.7 langfuse\n",
        "!pip -q install pydantic instructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzwe9BQOSwAQ",
        "outputId": "4675e967-fb44-404b-fa81-b329f59fdace",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ee5fdfa7050>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor, Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "import instructor\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
        "\n",
        "# Repro-ish (note: generation still has stochasticity unless you fix sampling params)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Langfuse"
      ],
      "metadata": {
        "id": "9Bqa-e4BZbvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse import Langfuse\n",
        "os.environ['LC_PK'] = userdata.get('LC_PK')\n",
        "os.environ['LC_SK'] = userdata.get('LC_SK')\n",
        "os.environ['LC_H'] = userdata.get('LC_H')\n",
        "langfuse = Langfuse(\n",
        "    public_key=os.environ['LC_PK'],\n",
        "    secret_key=os.environ['LC_SK'],\n",
        "    host=os.environ['LC_H']\n",
        ")"
      ],
      "metadata": {
        "id": "KNbx9u8OZIQH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Connected:\", langfuse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bROEmJxNZbOt",
        "outputId": "a5276453-8e53-41b9-e7e7-2aa68377d765"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected: <langfuse._client.client.Langfuse object at 0x7ee4547479e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Model and Dataset"
      ],
      "metadata": {
        "id": "FJQnBUmlZea0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(MODEL_ID,\n",
        "#                                                            device_map=\"auto\", dtype=torch.bfloat16,)\n",
        "model = Qwen2AudioForConditionalGeneration.from_pretrained(MODEL_ID,\n",
        "                                                           device_map=\"auto\",\n",
        "                                                           dtype=torch.bfloat16,)\n",
        "\n",
        "sr = processor.feature_extractor.sampling_rate\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "FER0uytDqVTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969,
          "referenced_widgets": [
            "2f6b6d7fc8394644a51a8052dabd5fd4",
            "2d863d8c337b4ef88b0e11ec2e63ee61",
            "a1efdef8dfc5400daf2a0f9db4883956",
            "1c6f6d70b068415fad07c0837fa46a48",
            "726a9ad7070743eba7a14ea21513affa",
            "03b48c8664f54eeca59b3c54b3eedb03",
            "6a78524d94934d13894ad24607972888",
            "6e4f051363d244c9a369fca50cecc6fa",
            "71215ae0ec1543ce85aac1e81498fa33",
            "8971d811eb1a42048c39116cd77fb621",
            "b8c01b5f7b2d4778a13242a74a148cbc"
          ]
        },
        "outputId": "0b10714e-d277-4da3-e750-0e29a391fab2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f6b6d7fc8394644a51a8052dabd5fd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2AudioForConditionalGeneration(\n",
              "  (audio_tower): Qwen2AudioEncoder(\n",
              "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "    (embed_positions): Embedding(1500, 1280)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x Qwen2AudioEncoderLayer(\n",
              "        (self_attn): Qwen2AudioAttention(\n",
              "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation_fn): GELUActivation()\n",
              "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
              "  )\n",
              "  (multi_modal_projector): Qwen2AudioMultiModalProjector(\n",
              "    (linear): Linear(in_features=1280, out_features=4096, bias=True)\n",
              "  )\n",
              "  (language_model): Qwen2ForCausalLM(\n",
              "    (model): Qwen2Model(\n",
              "      (embed_tokens): Embedding(156032, 4096)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x Qwen2DecoderLayer(\n",
              "          (self_attn): Qwen2Attention(\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (mlp): Qwen2MLP(\n",
              "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
              "          (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
              "      (rotary_emb): Qwen2RotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=4096, out_features=156032, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_id)\n",
        "chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "    chat_model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "96a08d39d69e403684cdb36ccc2d0c2a",
            "e155ed124b5a4ae28c8cf92c3d9d7f55",
            "ab5d11c879bc4a28819e59e617ad14c2",
            "2bf4027fb995406a999e3e63abf9033b",
            "e8eb607651344f1496d7b09cb2bc99dd",
            "3dcd1da03fdb46d28532689e6aa83e6f",
            "f091badb4e5647fc88fad6baea30849b",
            "3fe3b83f86d84ad5bb7b10a02f80989b",
            "64344ad616a84233915efd2281695ea0",
            "a7b83e017f2747ca836b9a2e943ef416",
            "9dd8fd1d8d4145a5b860bdcbf4103869"
          ]
        },
        "id": "-vBA44SFS3H4",
        "outputId": "9ada0f04-62bf-4f0f-b235-0acab24e73f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96a08d39d69e403684cdb36ccc2d0c2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "#define OpenAi api styled class for instructor output\n",
        "class MockMessage:\n",
        "    def __init__(self, content: str, role: str = \"assistant\"):\n",
        "        self.role = role\n",
        "        self.content = content\n",
        "\n",
        "class MockChoice:\n",
        "    def __init__(self, text: str, index: int = 0):\n",
        "        self.index = index\n",
        "        self.message = MockMessage(content=text)\n",
        "        self.finish_reason = \"stop\"\n",
        "\n",
        "class MockCompletion:\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str,\n",
        "        model: str = \"local-llm\",\n",
        "        prompt_tokens: int = 0,\n",
        "        completion_tokens: int = 0,\n",
        "    ):\n",
        "        self.id = f\"chatcmpl-local-{int(time.time())}\"\n",
        "        self.object = \"chat.completion\"\n",
        "        self.created = int(time.time())\n",
        "        self.model = model\n",
        "        self.choices: List[MockChoice] = [MockChoice(text)]\n",
        "        self.usage: Dict[str, Any] = {\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens,\n",
        "            \"total_tokens\": prompt_tokens + completion_tokens,\n",
        "        }"
      ],
      "metadata": {
        "id": "rNu25gNlbHDs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# reponse string sanitization helper\n",
        "def strip_code_fences(text: str) -> str:\n",
        "    \"\"\"Remove Markdown ```json ... ``` or ``` blocks.\"\"\"\n",
        "    # Remove triple backticks and language hints\n",
        "    text = re.sub(r\"^```(?:json|JSON)?\", \"\", text.strip())\n",
        "    text = re.sub(r\"```$\", \"\", text.strip())\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "-4ZLJOXFehRg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=chat_model, tokenizer=chat_tokenizer)\n",
        "\n",
        "def mistral_create(messages, **kwargs):\n",
        "    prompt = \"\\n\".join([m[\"content\"] for m in messages])\n",
        "    out = generator(prompt)[0][\"generated_text\"]\n",
        "    out = strip_code_fences(out)\n",
        "    return MockCompletion(\n",
        "        text=out,\n",
        "        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    )\n",
        "\n",
        "\n",
        "mistral_structured = instructor.patch(\n",
        "    create=mistral_create,\n",
        "    mode=instructor.Mode.JSON_SCHEMA,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZUBa_2rWo8m",
        "outputId": "6e2e9ea3-7498-4628-e414-2e3f9d9424fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"vanderbilt-dsi/sep-28k-extended\", token=os.getenv(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "lpjCoiVEqqTL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a multimodal model trained to detect stuttering disfluencies in both adults and children.\n",
        "You analyze the provided audio waveform and its transcript to identify disfluencies with high precision.\n",
        "\n",
        "Task:\n",
        "Detect and label any stuttering disfluencies in the input audio‚Äìtext pair.\n",
        "Use both acoustic cues (pauses, prolongations, effort) and linguistic patterns (repetition, interjection, revision).\n",
        "\n",
        "Definitions:\n",
        "- Repetition: Unintentional repeating of sounds, syllables, or words (e.g., \"b-b-ball\").\n",
        "- Prolongation: A sound held longer than normal (e.g., \"ssssun\").\n",
        "- Block: A stoppage of airflow or voicing before or during speech (e.g., \"‚Äî‚Äîbook\").\n",
        "\n",
        "Output format rules:\n",
        "- Output **only** valid JSON ‚Äî no extra text, explanations, or comments.\n",
        "- Use **double quotes** around all keys and string values.\n",
        "- Output must be a JSON **array** of objects, not Python-style dicts.\n",
        "- Numeric fields must be valid floats between 0 and 1 for confidence_score.\n",
        "- Each object must follow this exact structure:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"time_start\": <float>,\n",
        "    \"time_end\": <float>,\n",
        "    \"transcript_segment\": \"<text>\",\n",
        "    \"disfluency_type\": \"<repetition | prolongation | block>\",\n",
        "    \"confidence_score\": <float 0-1>,\n",
        "    \"commentary\": \"<optional reasoning or null>\"\n",
        "  }\n",
        "]\n",
        "\n",
        "If no disfluencies are detected, output `[]` (an empty JSON array) and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "question_prompt = \"\"\"\n",
        "Analyze the following audio and transcript.\n",
        "Identify all stuttering disfluencies and return them in the required JSON format.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "4VyiCa8bWiLp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, RootModel\n",
        "from typing import List, Optional\n",
        "\n",
        "#for local instruct or pydantic\n",
        "class Disfluency(BaseModel):\n",
        "    time_start: float\n",
        "    time_end: float\n",
        "    transcript_segment: str\n",
        "    disfluency_type: str = Field(..., pattern=\"repetition|prolongation|block\")\n",
        "    confidence_score: float = Field(..., ge=0, le=1)\n",
        "    commentary: Optional[str] = None\n",
        "\n",
        "# class DisfluencyResults(RootModel[List[Disfluency]]):\n",
        "#     pass\n",
        "class DisfluencyResults(BaseModel):\n",
        "    results: List[Disfluency]"
      ],
      "metadata": {
        "id": "HXT1h8fIKuAw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for function calling: more available through remote client apis\n",
        "schema = {\n",
        "    \"name\": \"DisfluencyResults\",\n",
        "    \"schema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"results\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"time_start\": {\"type\": \"number\"},\n",
        "                        \"time_end\": {\"type\": \"number\"},\n",
        "                        \"transcript_segment\": {\"type\": \"string\"},\n",
        "                        \"disfluency_type\": {\"type\": \"string\", \"enum\": [\"repetition\", \"prolongation\", \"block\"]},\n",
        "                        \"confidence_score\": {\"type\": \"number\"},\n",
        "                        \"commentary\": {\"type\": \"string\"}\n",
        "                    },\n",
        "                    \"required\": [\"time_start\", \"time_end\", \"transcript_segment\", \"disfluency_type\", \"confidence_score\"]\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"results\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "czmYnMc2TxMn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define langfuse observation"
      ],
      "metadata": {
        "id": "vgh_SQzndFCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pydantic import ValidationError\n",
        "from instructor.exceptions import InstructorRetryException\n",
        "\n",
        "def safe_structured_parse(messages, response_model):\n",
        "    try:\n",
        "        structured = mistral_structured(\n",
        "            messages=messages,\n",
        "            response_model=response_model,\n",
        "        )\n",
        "        return structured\n",
        "\n",
        "    except InstructorRetryException as e:\n",
        "        print(\"\\n‚ö†Ô∏è InstructorRetryException triggered!\")\n",
        "        print(f\"Exception type: {type(e).__name__}\")\n",
        "        print(f\"Raw exception message:\\n{e}\\n\")\n",
        "        if hasattr(e, \"args\") and e.args:\n",
        "            print(\"Args passed to exception:\", e.args)\n",
        "        raise  # re-raise for now; remove this line if you want to continue execution\n",
        "\n",
        "    except ValidationError as e:\n",
        "        print(\"\\n‚ùå Pydantic ValidationError:\")\n",
        "        print(e)\n",
        "        # Try to extract invalid JSON from the input value\n",
        "        try:\n",
        "            invalid_json = e.errors()[0].get(\"input_value\", \"\")\n",
        "            print(\"\\n--- Invalid JSON string ---\")\n",
        "            print(invalid_json[:500])  # print a snippet if long\n",
        "            print(\"--- end ---\\n\")\n",
        "            # Optionally save to a temp file for inspection\n",
        "            with open(\"failed_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(invalid_json)\n",
        "                print(\"üßæ Saved problematic output to failed_output.json\")\n",
        "        except Exception as inner:\n",
        "            print(\"Couldn't extract invalid JSON:\", inner)\n",
        "        raise  # optional: comment out if you want graceful fallback\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"\\n‚ùå JSONDecodeError:\")\n",
        "        print(f\"Error message: {e}\")\n",
        "        print(f\"Line {e.lineno}, Column {e.colno}\")\n",
        "        raise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nüö® Unexpected error in structured parse:\")\n",
        "        print(f\"Type: {type(e).__name__}\")\n",
        "        print(f\"Message: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "4gLJmWBXfjOI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse import observe\n",
        "import time, torch, json, instructor\n",
        "from pydantic import ValidationError\n",
        "\n",
        "@observe(name=\"Qwen2Audio-stutter-pipeline\")\n",
        "def run_stuttering_detection_pipeline(model, processor, sample, system_prompt, question_prompt):\n",
        "    \"\"\"\n",
        "    Full inference pipeline: takes audio sample and prompts,\n",
        "    runs Qwen2Audio model, and returns JSON output.\n",
        "    \"\"\"\n",
        "    # Build chat structure\n",
        "    conversation = [\n",
        "        {'role': 'system', 'content': system_prompt},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"audio\", \"audio\": sample},\n",
        "            {\"type\": \"text\", \"text\": question_prompt},\n",
        "        ]},\n",
        "    ]\n",
        "\n",
        "    # Step 1: Prepare text + audio\n",
        "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
        "    audios = [sample[\"array\"]]\n",
        "\n",
        "    # Step 2: Tokenize & prepare tensors\n",
        "    inputs = processor(text=text, audio=audios, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    # print(f\"#audio segments: {len(audios)} | input_ids shape: {inputs['input_ids'].shape}\")\n",
        "\n",
        "    # Step 3: Inference\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        generate_ids = model.generate(**inputs, max_length=1024)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    # Step 4: Decode\n",
        "    text_id = generate_ids\n",
        "    response_ids = text_id[:, inputs[\"input_ids\"].size(1):]\n",
        "    response = processor.batch_decode(\n",
        "        response_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    print(response)\n",
        "\n",
        "    structured = safe_structured_parse(\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a strict JSON emitter. \"\n",
        "            \"Output strictly valid JSON. Use double quotes for all keys and string values. \"\n",
        "            \"Do not wrap output in code fences or Python-style dicts. \"\n",
        "            \"Return a JSON array of Disfluency objects. If no disfluencies are detected, return an empty array `[]`.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Convert this into valid DisfluencyResults JSON:\\n{response}\"}\n",
        "    ],\n",
        "      response_model=DisfluencyResults,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"‚è±Ô∏è Generation took {elapsed:.2f}s\")\n",
        "    return structured"
      ],
      "metadata": {
        "id": "hB-wbLeDbTvl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define visualization method"
      ],
      "metadata": {
        "id": "WVADiISWdH3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def play_audio_via_get_all_samples(dataset, index=0, plot=True, play=True):\n",
        "    \"\"\"\n",
        "    Decode, play, and visualize an audio clip using torchcodec's get_all_samples().\n",
        "    Displays waveform (top) and spectrogram (bottom).\n",
        "\n",
        "    Assumptions:\n",
        "      dataset is Huggingface object with audio column as decoded Torchcodec object\n",
        "      Torchcodec 0.7\n",
        "\n",
        "    \"\"\"\n",
        "    # Handle dataset split\n",
        "    if hasattr(dataset, \"keys\") and \"train\" in dataset:\n",
        "        sample = dataset[\"train\"][index]\n",
        "    else:\n",
        "        sample = dataset[index]\n",
        "\n",
        "    audio = sample[\"audio\"]\n",
        "\n",
        "    # Decode via torchcodec\n",
        "    samples = audio.get_all_samples()\n",
        "    waveform = samples.data\n",
        "    sr = samples.sample_rate\n",
        "\n",
        "    if isinstance(waveform, torch.Tensor):\n",
        "        waveform = waveform.squeeze().cpu().numpy()\n",
        "\n",
        "    if plot:\n",
        "        # Prepare figure\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "        # --- Waveform ---\n",
        "        ax1.plot(np.arange(len(waveform)) / sr, waveform, linewidth=0.8)\n",
        "        ax1.set_title(f\"Waveform (index={index}, {sr} Hz)\")\n",
        "        ax1.set_xlabel(\"Time (s)\")\n",
        "        ax1.set_ylabel(\"Amplitude\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # --- Spectrogram ---\n",
        "        Pxx, freqs, bins, im = ax2.specgram(\n",
        "            waveform,\n",
        "            NFFT=1024,\n",
        "            Fs=sr,\n",
        "            noverlap=512,\n",
        "            cmap=\"magma\"\n",
        "        )\n",
        "        ax2.set_title(\"Spectrogram\")\n",
        "        ax2.set_xlabel(\"Time (s)\")\n",
        "        ax2.set_ylabel(\"Frequency (Hz)\")\n",
        "        # fig.colorbar(im, ax=ax2, label=\"Intensity (dB)\")\n",
        "\n",
        "        # plt.legend(None)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Play in notebook\n",
        "    if play:\n",
        "        display(Audio(waveform, rate=sr))\n",
        "\n",
        "    return waveform, sr"
      ],
      "metadata": {
        "id": "SUYf2w6Mb7fN",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call function"
      ],
      "metadata": {
        "id": "2jwNw_xQdQbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = \"[{\\\"time_start\\\": \\\"0.58\\\", \\\"time_end\\\": \\\"1.24\\\", \\\"transcript_segment\\\": \\\"myself\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.7\\\", \\\"commentary\\\": \\\"Short stuttering hesitation.\\\"}, {\\\"time_start\\\": \\\"1.63\\\", \\\"time_end\\\": \\\"2.09\\\", \\\"transcript_segment\\\": \\\"limiting\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.8\\\", \\\"commentary\\\": \\\"Brief stuttering hesitation.\\\"}]\"\n",
        "\n",
        "structured = safe_structured_parse(\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a strict JSON emitter. \"\n",
        "            \"Output strictly valid JSON. Use double quotes for all keys and string values. \"\n",
        "            \"Do not wrap output in code fences or Python-style dicts. \"\n",
        "            \"Return a JSON array of Disfluency objects. If no disfluencies are detected, return an empty array `[]`.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Convert this into valid DisfluencyResults JSON:\\n{response}\"}\n",
        "    ],\n",
        "      response_model=DisfluencyResults,\n",
        "    )"
      ],
      "metadata": {
        "id": "w5cqy_pThPvY",
        "outputId": "a7bdcfd2-9abe-4f62-fc34-e669f6de50b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö†Ô∏è InstructorRetryException triggered!\n",
            "Exception type: InstructorRetryException\n",
            "Raw exception message:\n",
            "<failed_attempts>\n",
            "\n",
            "<generation number=\"1\">\n",
            "<exception>\n",
            "    1 validation error for DisfluencyResults\n",
            "  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "</exception>\n",
            "<completion>\n",
            "    <__main__.MockCompletion object at 0x7ee06c868950>\n",
            "</completion>\n",
            "</generation>\n",
            "\n",
            "</failed_attempts>\n",
            "\n",
            "<last_exception>\n",
            "    1 validation error for DisfluencyResults\n",
            "  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "</last_exception>\n",
            "\n",
            "Args passed to exception: (1 validation error for DisfluencyResults\n",
            "  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InstructorRetryException",
          "evalue": "<failed_attempts>\n\n<generation number=\"1\">\n<exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</exception>\n<completion>\n    <__main__.MockCompletion object at 0x7ee06c868950>\n</completion>\n</generation>\n\n</failed_attempts>\n\n<last_exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</last_exception>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     )\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     return process_response(  # type: ignore\n\u001b[0m\u001b[1;32m    200\u001b[0m                         \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/response.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(response, response_model, stream, validation_context, strict, mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     model = response_model.from_response(  # type: ignore\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36mfrom_response\u001b[0;34m(cls, completion, validation_context, strict, mode)\u001b[0m\n\u001b[1;32m    238\u001b[0m         }:\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36mparse_json\u001b[0;34m(cls, completion, validation_context, strict)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# Validate the model from the JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_model_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36m_validate_model_from_json\u001b[0;34m(cls, json_str, validation_context, strict)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             return cls.model_validate_json(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36mmodel_validate_json\u001b[0;34m(cls, json_data, strict, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         return cls.__pydantic_validator__.validate_json(\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_alias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_alias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for DisfluencyResults\n  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7edb9cbc4ec0 state=finished raised ValidationError>]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-744714958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[{\\\"time_start\\\": \\\"0.58\\\", \\\"time_end\\\": \\\"1.24\\\", \\\"transcript_segment\\\": \\\"myself\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.7\\\", \\\"commentary\\\": \\\"Short stuttering hesitation.\\\"}, {\\\"time_start\\\": \\\"1.63\\\", \\\"time_end\\\": \\\"2.09\\\", \\\"transcript_segment\\\": \\\"limiting\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.8\\\", \\\"commentary\\\": \\\"Brief stuttering hesitation.\\\"}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m structured = safe_structured_parse(\n\u001b[0m\u001b[1;32m      4\u001b[0m       messages=[\n\u001b[1;32m      5\u001b[0m         {\"role\": \"system\", \"content\": (\n",
            "\u001b[0;32m/tmp/ipython-input-1787433720.py\u001b[0m in \u001b[0;36msafe_structured_parse\u001b[0;34m(messages, response_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_structured_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         structured = mistral_structured(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/patch.py\u001b[0m in \u001b[0;36mnew_create_sync\u001b[0;34m(response_model, validation_context, context, max_retries, strict, hooks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         response = retry_sync(\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retry error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         raise InstructorRetryException(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlast_completion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m: <failed_attempts>\n\n<generation number=\"1\">\n<exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</exception>\n<completion>\n    <__main__.MockCompletion object at 0x7ee06c868950>\n</completion>\n</generation>\n\n</failed_attempts>\n\n<last_exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: trailing characters at line 1 column 181 [type=json_invalid, input_value='{\"time_start\": \"0.58\", \"...ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</last_exception>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"train\"][\"audio\"][0]\n",
        "\n",
        "response = run_stuttering_detection_pipeline(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    sample=sample,\n",
        "    system_prompt=system_prompt,\n",
        "    question_prompt=question_prompt\n",
        ")\n",
        "\n",
        "print(\"\\n=== MODEL RESPONSE ===\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FHNGdwx4cHf4",
        "outputId": "79942741-93bd-4e94-ff97-9cfcb301a1b5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\\\"time_start\\\": \\\"0.58\\\", \\\"time_end\\\": \\\"1.24\\\", \\\"transcript_segment\\\": \\\"myself\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.7\\\", \\\"commentary\\\": \\\"Short stuttering hesitation.\\\"}, {\\\"time_start\\\": \\\"1.63\\\", \\\"time_end\\\": \\\"2.09\\\", \\\"transcript_segment\\\": \\\"limiting\\\", \\\"disfluency_type\\\": \\\"stuttering\\\", \\\"confidence_score\\\": \\\"0.8\\\", \\\"commentary\\\": \\\"Brief stuttering hesitation.\\\"}]\n",
            "\n",
            "‚ö†Ô∏è InstructorRetryException triggered!\n",
            "Exception type: InstructorRetryException\n",
            "Raw exception message:\n",
            "<failed_attempts>\n",
            "\n",
            "<generation number=\"1\">\n",
            "<exception>\n",
            "    1 validation error for DisfluencyResults\n",
            "  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "</exception>\n",
            "<completion>\n",
            "    <__main__.MockCompletion object at 0x7ee06caa39b0>\n",
            "</completion>\n",
            "</generation>\n",
            "\n",
            "</failed_attempts>\n",
            "\n",
            "<last_exception>\n",
            "    1 validation error for DisfluencyResults\n",
            "  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "</last_exception>\n",
            "\n",
            "Args passed to exception: (1 validation error for DisfluencyResults\n",
            "  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InstructorRetryException",
          "evalue": "<failed_attempts>\n\n<generation number=\"1\">\n<exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</exception>\n<completion>\n    <__main__.MockCompletion object at 0x7ee06caa39b0>\n</completion>\n</generation>\n\n</failed_attempts>\n\n<last_exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</last_exception>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     )\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     return process_response(  # type: ignore\n\u001b[0m\u001b[1;32m    200\u001b[0m                         \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/response.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(response, response_model, stream, validation_context, strict, mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     model = response_model.from_response(  # type: ignore\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36mfrom_response\u001b[0;34m(cls, completion, validation_context, strict, mode)\u001b[0m\n\u001b[1;32m    238\u001b[0m         }:\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36mparse_json\u001b[0;34m(cls, completion, validation_context, strict)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# Validate the model from the JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_model_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/processing/function_calls.py\u001b[0m in \u001b[0;36m_validate_model_from_json\u001b[0;34m(cls, json_str, validation_context, strict)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             return cls.model_validate_json(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36mmodel_validate_json\u001b[0;34m(cls, json_data, strict, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         return cls.__pydantic_validator__.validate_json(\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_alias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_alias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for DisfluencyResults\n  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7ee45200d070 state=finished raised ValidationError>]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3345917535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m response = run_stuttering_detection_pipeline(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langfuse/_client/observe.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                         )\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m                     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_return_type_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langfuse/_client/observe.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcapture_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1446005168.py\u001b[0m in \u001b[0;36mrun_stuttering_detection_pipeline\u001b[0;34m(model, processor, sample, system_prompt, question_prompt)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     structured = safe_structured_parse(\n\u001b[0m\u001b[1;32m     48\u001b[0m       messages=[\n\u001b[1;32m     49\u001b[0m         {\"role\": \"system\", \"content\": (\n",
            "\u001b[0;32m/tmp/ipython-input-1787433720.py\u001b[0m in \u001b[0;36msafe_structured_parse\u001b[0;34m(messages, response_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_structured_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         structured = mistral_structured(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/patch.py\u001b[0m in \u001b[0;36mnew_create_sync\u001b[0;34m(response_model, validation_context, context, max_retries, strict, hooks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         response = retry_sync(\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retry error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         raise InstructorRetryException(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlast_completion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m: <failed_attempts>\n\n<generation number=\"1\">\n<exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</exception>\n<completion>\n    <__main__.MockCompletion object at 0x7ee06caa39b0>\n</completion>\n</generation>\n\n</failed_attempts>\n\n<last_exception>\n    1 validation error for DisfluencyResults\n  Invalid JSON: key must be a string at line 1 column 2 [type=json_invalid, input_value='{\\\\\"time_start\\\\\": \\\\\"0....ering hesitation.\"\\n  }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</last_exception>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This needs smaller prompt for Qwen"
      ],
      "metadata": {
        "id": "Q94Z7KUOs5_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filtering dataset to only stuttering sample"
      ],
      "metadata": {
        "id": "AfpMIimztFzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset['train'] = dataset['train'].add_column(\"idx\", list(range(len(dataset['train']))))\n",
        "\n",
        "# df = dataset['train'].to_pandas()\n",
        "# # Based on Ida's code\n",
        "# # Filter the DataFrame based on the specified criteria using OR and string comparison\n",
        "# filtered_df = df[(df['Prolongation'] == '3') | (df['SoundRep'] == '3') | (df['WordRep'] == '3') | (df['Block'] == '3')]\n",
        "# indices = filtered_df.index.tolist()\n",
        "\n",
        "# filtered_dataset = dataset['train'].select(indices)\n"
      ],
      "metadata": {
        "id": "uBuSPwzctJkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "WBqRgLbAtCj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# import json, os, time\n",
        "\n",
        "# # === SETTINGS ===\n",
        "# output_path = \"stuttering_results.jsonl\"\n",
        "# save_every = 5        # how often to checkpoint\n",
        "# max_retries = 3       # how many times to retry a failed item\n",
        "\n",
        "# # === LOAD EXISTING PROGRESS (if any) ===\n",
        "# completed_ids = set()\n",
        "# if os.path.exists(output_path):\n",
        "#     with open(output_path, \"r\") as f:\n",
        "#         for line in f:\n",
        "#             try:\n",
        "#                 record = json.loads(line)\n",
        "#                 completed_ids.add(record[\"idx\"])\n",
        "#             except json.JSONDecodeError:\n",
        "#                 continue\n",
        "#     print(f\"Resuming ‚Äî found {len(completed_ids)} completed items\")\n",
        "\n",
        "# # === RUN SAFELY ===\n",
        "# for item in tqdm(filtered_dataset):\n",
        "#     idx = item[\"idx\"]\n",
        "\n",
        "#     # Skip if already done\n",
        "#     if idx in completed_ids:\n",
        "#         continue\n",
        "\n",
        "#     sample = item[\"audio\"]\n",
        "\n",
        "#     # Retry wrapper for transient failures\n",
        "#     for attempt in range(max_retries):\n",
        "#         try:\n",
        "#             response = run_stuttering_detection_pipeline(\n",
        "#                 model=model,\n",
        "#                 processor=processor,\n",
        "#                 sample=sample,\n",
        "#                 system_prompt=system_prompt,\n",
        "#                 question_prompt=question_prompt,\n",
        "#             )\n",
        "\n",
        "#             record = {\n",
        "#                 \"idx\": idx,\n",
        "#                 \"response\": response,\n",
        "#                 \"timestamp\": time.time(),\n",
        "#             }\n",
        "\n",
        "#             # Save incrementally\n",
        "#             with open(output_path, \"a\") as f:\n",
        "#                 f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "#             completed_ids.add(idx)\n",
        "#             break  # success ‚Üí move to next sample\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"‚ö†Ô∏è Error on idx={idx} (attempt {attempt+1}/{max_retries}): {e}\")\n",
        "#             time.sleep(2)\n",
        "#             if attempt + 1 == max_retries:\n",
        "#                 # Log failure for later inspection\n",
        "#                 with open(\"stuttering_failures.log\", \"a\") as ef:\n",
        "#                     ef.write(f\"{idx}\\t{str(e)}\\n\")\n"
      ],
      "metadata": {
        "id": "dHLH1igktDUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f6b6d7fc8394644a51a8052dabd5fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d863d8c337b4ef88b0e11ec2e63ee61",
              "IPY_MODEL_a1efdef8dfc5400daf2a0f9db4883956",
              "IPY_MODEL_1c6f6d70b068415fad07c0837fa46a48"
            ],
            "layout": "IPY_MODEL_726a9ad7070743eba7a14ea21513affa"
          }
        },
        "2d863d8c337b4ef88b0e11ec2e63ee61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b48c8664f54eeca59b3c54b3eedb03",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a78524d94934d13894ad24607972888",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "a1efdef8dfc5400daf2a0f9db4883956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e4f051363d244c9a369fca50cecc6fa",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71215ae0ec1543ce85aac1e81498fa33",
            "value": 5
          }
        },
        "1c6f6d70b068415fad07c0837fa46a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8971d811eb1a42048c39116cd77fb621",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b8c01b5f7b2d4778a13242a74a148cbc",
            "value": "‚Äá5/5‚Äá[00:06&lt;00:00,‚Äá‚Äá1.04s/it]"
          }
        },
        "726a9ad7070743eba7a14ea21513affa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b48c8664f54eeca59b3c54b3eedb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a78524d94934d13894ad24607972888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e4f051363d244c9a369fca50cecc6fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71215ae0ec1543ce85aac1e81498fa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8971d811eb1a42048c39116cd77fb621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c01b5f7b2d4778a13242a74a148cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96a08d39d69e403684cdb36ccc2d0c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e155ed124b5a4ae28c8cf92c3d9d7f55",
              "IPY_MODEL_ab5d11c879bc4a28819e59e617ad14c2",
              "IPY_MODEL_2bf4027fb995406a999e3e63abf9033b"
            ],
            "layout": "IPY_MODEL_e8eb607651344f1496d7b09cb2bc99dd"
          }
        },
        "e155ed124b5a4ae28c8cf92c3d9d7f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dcd1da03fdb46d28532689e6aa83e6f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f091badb4e5647fc88fad6baea30849b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ab5d11c879bc4a28819e59e617ad14c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe3b83f86d84ad5bb7b10a02f80989b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64344ad616a84233915efd2281695ea0",
            "value": 3
          }
        },
        "2bf4027fb995406a999e3e63abf9033b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b83e017f2747ca836b9a2e943ef416",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9dd8fd1d8d4145a5b860bdcbf4103869",
            "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.65s/it]"
          }
        },
        "e8eb607651344f1496d7b09cb2bc99dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dcd1da03fdb46d28532689e6aa83e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f091badb4e5647fc88fad6baea30849b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fe3b83f86d84ad5bb7b10a02f80989b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64344ad616a84233915efd2281695ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7b83e017f2747ca836b9a2e943ef416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd8fd1d8d4145a5b860bdcbf4103869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}